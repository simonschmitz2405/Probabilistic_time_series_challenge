{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\.conda\\envs\\ptsc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.BikeDataImporter import BikeDataImporter\n",
    "from data.DataPreparing import DataPreparing\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from suntime import Sun, SunTimeException\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sktime.split import ExpandingSlidingWindowSplitter\n",
    "from sklearn.metrics import d2_pinball_score\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import MSTL\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"stylesheet.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bike_and_weather_data():\n",
    "    \"\"\"Get the bike data and the weather data as a single dataframe.\n",
    "\n",
    "    Returns:\n",
    "        data_w_weather (pd.DataFrame): The bike data with the weather data.\n",
    "    \"\"\"\n",
    "    bike_data = get_bike_data()\n",
    "    # Give the last date of the bike data\n",
    "    weather_data_hourly = get_weather_data_hourly()\n",
    "    weather_data_daily = get_weather_data_daily(weather_data_hourly)\n",
    "    data_w_weather = combine_data_weather(bike_data, weather_data_daily)\n",
    "    # Filter the data to the last date of the bike data\n",
    "\n",
    "    return data_w_weather\n",
    "\n",
    "def get_bike_data():\n",
    "    \"\"\"Call the BikeDataImporter to get the bike data.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): The start date for the data.\n",
    "\n",
    "    Returns:\n",
    "        data (pd.Dataframe): Bike counts for each day.\n",
    "    \"\"\"\n",
    "    bike_data = BikeDataImporter()\n",
    "    data = bike_data.get_bike_data()\n",
    "    return data\n",
    "\n",
    "def get_weather_data_hourly():\n",
    "    \"\"\"Get the weather data for Karlsruhe from the historical and forecast data on hourly basis.\n",
    "\n",
    "    Returns:\n",
    "        weather_data (pd.Dataframe): Hourly historical and forecast weather data.\n",
    "    \"\"\"\n",
    "    todays_date = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
    "    # Load the historical and the forecast weather data\n",
    "    history_data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"weather\", \"history\", \"karlsruhe\", f\"history_karlsruhe_{todays_date}.csv\")\n",
    "    forecast_data_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"weather\", \"forecasts\", \"karlsruhe\", f\"forecast_karlsruhe_{todays_date}.csv\")\n",
    "    \n",
    "    # Read in the historical and forecast data\n",
    "    history_data = pd.read_csv(history_data_path)\n",
    "    forecast_data = pd.read_csv(forecast_data_path)\n",
    "\n",
    "    # Set the index to the date\n",
    "    history_data.set_index('date', inplace=True)\n",
    "    forecast_data.set_index('date', inplace=True)\n",
    "\n",
    "    # Convert the index to datetime\n",
    "    history_data.index = pd.to_datetime(history_data.index)\n",
    "    forecast_data.index = pd.to_datetime(forecast_data.index)\n",
    "\n",
    "    # Convert the index to datetime\n",
    "    history_data.index = history_data.index.tz_convert('UTC')\n",
    "    forecast_data.index = forecast_data.index.tz_convert('UTC')\n",
    "\n",
    "    # Convert the index to European timezone (e.g., Europe/Paris)\n",
    "    # history_data.index = history_data.index.tz_convert('Europe/Paris')\n",
    "    # forecast_data.index = forecast_data.index.tz_convert('Europe/Paris')\n",
    "\n",
    "    # The history data is not complete, so we need to filter it\n",
    "    max_hisory_date = pd.Timestamp.now(tz='UTC') - pd.Timedelta(days=2)\n",
    "    history_data_filtered = history_data.loc[(history_data.index < max_hisory_date )].copy()\n",
    "    forecast_data_filtered = forecast_data.loc[(forecast_data.index >= max_hisory_date)].copy()\n",
    "\n",
    "    # Merge the historical and forecast data\n",
    "    weather_data = pd.concat([history_data_filtered, forecast_data_filtered], axis=0)\n",
    "\n",
    "    return weather_data.iloc[1:]\n",
    "\n",
    "def get_weather_data_daily(weather_data_hourly):\n",
    "    \"\"\"Resample the hourly weather data to daily data.\n",
    "\n",
    "    Args:\n",
    "        weather_data_hourly (pd.Dataframe): Hourly historical and forecast weather data.\n",
    "\n",
    "    Returns:\n",
    "        weather_data_daily (pd.Dataframe): Daily historical and forecast weather data.\n",
    "    \"\"\"\n",
    "    weather_data_daily = weather_data_hourly.resample('D').mean() # Todo: Check if we need to resample differently\n",
    "    return weather_data_daily\n",
    "\n",
    "def combine_data_weather(bike_data, weather_data):\n",
    "    \"\"\"Merge the bike data with the weather data.\n",
    "\n",
    "    Args:\n",
    "        bike_data (pd.Dataframe): Daily bike data.\n",
    "        weather_data (pd.Dataframe): Daily weather data.\n",
    "\n",
    "    Returns:\n",
    "        bike_and_weather_data (pd.Dataframe): bike_and_weather_data\n",
    "    \"\"\"\n",
    "    bike_and_weather_data = pd.merge(bike_data, weather_data, left_index=True, right_index=True, how='left')\n",
    "    return bike_and_weather_data\n",
    "  \n",
    "def create_features(data):\n",
    "    \"\"\"Create the features based on the data.\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "\n",
    "    # Create time features\n",
    "    data['month'] = data.index.month\n",
    "    data['day'] = data.index.day\n",
    "    data['quarter'] = data.index.quarter\n",
    "    data['weekday'] = data.index.weekday\n",
    "\n",
    "    # Create lag features\n",
    "    target_map = data['bike_count'].to_dict()\n",
    "    # data['lag_1'] = (data.index - pd.Timedelta(days=1)).map(target_map) # Todo: Change to iterativ approach so that we can use these features as well.\n",
    "    # data['lag_2'] = (data.index - pd.Timedelta(days=2)).map(target_map)\n",
    "    # data['lag_3'] = (data.index - pd.Timedelta(days=3)).map(target_map)\n",
    "    # data['lag_4'] = (data.index - pd.Timedelta(days=4)).map(target_map)\n",
    "    data['lag_5'] = (data.index - pd.Timedelta(days=7)).map(target_map)\n",
    "    data['lag_6'] = (data.index - pd.Timedelta(days=14)).map(target_map)\n",
    "    data['lag_7'] = (data.index - pd.Timedelta(days=20)).map(target_map)\n",
    "    data['lag_8'] = (data.index - pd.Timedelta(days=28)).map(target_map)\n",
    "    data['lag_9'] = (data.index - pd.Timedelta(days=50)).map(target_map)\n",
    "\n",
    "    # Create holiday feature\n",
    "    data[\"is_holiday\"] = 0\n",
    "    bw_holidays = holidays.country_holidays(\"DE\", subdiv=\"BW\")\n",
    "    for i, date in enumerate(data.index):\n",
    "        if date in bw_holidays:\n",
    "            data.at[date, \"is_holiday\"] = 1\n",
    "\n",
    "    # Create corona feature\n",
    "    lockdown1_start = pd.Timestamp('2020-03-16', tz='UTC')\n",
    "    lockdown1_end = pd.Timestamp('2020-05-11', tz='UTC')\n",
    "    easing_start = pd.Timestamp('2020-06-01', tz='UTC')\n",
    "    easing_end = pd.Timestamp('2020-10-30', tz='UTC')\n",
    "    lockdown2_start = pd.Timestamp('2020-11-02', tz='UTC')\n",
    "    lockdown2_end = pd.Timestamp('2020-02-14', tz='UTC')\n",
    "    vaccionation_start = pd.Timestamp('2021-03-01', tz='UTC')\n",
    "    vaccionation_end = pd.Timestamp('2021-06-30', tz='UTC')\n",
    "\n",
    "    data.loc[(data.index < lockdown1_start), 'corona_phase'] = 4 # Pre-Pandemic\n",
    "    data.loc[(data.index >= lockdown1_start) & (data.index <= lockdown1_end), 'corona_phase'] = 0 # First Lockdown\n",
    "    data.loc[(data.index >= easing_start) & (data.index <= easing_end), 'corona_phase'] = 5 # Easing\n",
    "    data.loc[(data.index >= lockdown2_start) & (data.index <= lockdown2_end), 'corona_phase'] = 1 # Second Lockdown\n",
    "    data.loc[(data.index >= vaccionation_start) & (data.index <= vaccionation_end), 'corona_phase'] = 2 # Vaccination Rollout\n",
    "    data.loc[(data.index > vaccionation_end), 'corona_phase'] = 3 # Post-Vaccination\n",
    "\n",
    "\n",
    "    # Create rolling features\n",
    "    # data['rolling_mean_7'] = (data['bike_count'].rolling(window=7).mean())\n",
    "    # data['rolling_mean_30'] = (data['bike_count'].rolling(window=30).mean())\n",
    "    # data[\"rolling_std_7\"] = data[\"bike_count\"].rolling(window=7).std()\n",
    "    # data[\"rolling_std_30\"] = data[\"bike_count\"].rolling(window=30).std()\n",
    "    # data[\"diff_prev_day\"] = data[\"bike_count\"].diff()\n",
    "\n",
    "    return data\n",
    "\n",
    "data_preparer = DataPreparing()\n",
    "data = data_preparer.get_bike_and_weather_data()\n",
    "data = data_preparer.create_features(data) # TTODO Still add here the iterative approach for the lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation of the features with the target bike_count\n",
    "correlation = data.corr()\n",
    "correlation['bike_count'].sort_values(ascending=False).plot(kind='bar', figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the peasron correlation matrix\n",
    "correlation = data.corr()\n",
    "\n",
    "# Select from the data just some specific columns\n",
    "data_selected = data[\n",
    "    [\n",
    "        \"bike_count\",\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"snow_depth\",\n",
    "        \"month\",\n",
    "        \"quarter\",\n",
    "        \"weekday\",\n",
    "        \"lag_5\",\n",
    "        \"lag_6\",\n",
    "        \"lag_7\",\n",
    "        \"lag_8\",\n",
    "        \"lag_9\",\n",
    "        \"is_holiday\",\n",
    "        \"corona_phase\",\n",
    "    ]\n",
    "]\n",
    "plt.figure(figsize=(20, 10))\n",
    "correlation = data_selected.corr()\n",
    "sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data['bike_count']` contains the target variable\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(data['bike_count'], lags=30, ax=axes[0])\n",
    "axes[0].set_title(\"ACF\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Lags\", fontsize=12)\n",
    "axes[0].set_ylabel(\"ACF\", fontsize=12)\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(data['bike_count'], lags=30, ax=axes[1], method='ywm')\n",
    "axes[1].set_title(\"PACF\", fontsize=14)\n",
    "axes[1].set_xlabel(r\"Lags\", fontsize=12)\n",
    "axes[1].set_ylabel(\"PACF\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series data\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(data.index, data['bike_count'], label='Bike Count')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Bike Count')\n",
    "plt.title('Time Series of Bike Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-val-test\n",
    "# ==============================================================================\n",
    "data = data.loc['2012-01-01 00:00:00':'2014-12-30 23:00:00', :].copy()\n",
    "end_train = '2013-12-31 23:59:00'\n",
    "end_validation = '2014-11-30 23:59:00'\n",
    "data_train = data.loc[: end_train, :].copy()\n",
    "data_val   = data.loc[end_train:end_validation, :].copy()\n",
    "data_test  = data.loc[end_validation:, :].copy()\n",
    "\n",
    "print(f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\n",
    "print(f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\n",
    "print(f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n",
    "\n",
    "# Interactive plot of time series\n",
    "# ==============================================================================\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data_train.index, y=data_train['bike_count'], mode='lines', name='Train'))\n",
    "fig.add_trace(go.Scatter(x=data_val.index, y=data_val['bike_count'], mode='lines', name='Validation'))\n",
    "fig.add_trace(go.Scatter(x=data_test.index, y=data_test['bike_count'], mode='lines', name='Test'))\n",
    "fig.update_layout(\n",
    "    title  = 'Hourly energy demand',\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"bike_count\",\n",
    "    legend_title=\"Partition:\",\n",
    "    width=750,\n",
    "    height=370,\n",
    "    margin=dict(l=20, r=20, t=35, b=20),\n",
    "    legend=dict(orientation=\"h\", yanchor=\"top\", y=1, xanchor=\"left\", x=0.001)\n",
    ")\n",
    "#fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bike_count'].plot(kind='hist', bins=500, figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = data[['bike_count']]\n",
    "train = data_split.loc[data.index < '07-01-2024']\n",
    "test = data_split.loc[data.index >= '07-01-2024']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "train.plot(ax=ax, label='Train split', title='Bike Count')\n",
    "test.plot(ax=ax, label='Test split')\n",
    "ax.axvline('07-01-2024', color='black', linestyle='--', lw=2)\n",
    "plt.legend(['Train split', 'Test split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split.loc[(data.index > '01-01-2024') & (data.index < '02-01-2024')].plot(figsize=(15,5), title=\"Month of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split.loc[(data.index > '01-01-2024') & (data.index < '01-08-2024')].plot(figsize=(15,5), title=\"Week of data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=5, test_size=7, gap = 0)\n",
    "\n",
    "fig, axs = plt.subplots(5,1, figsize=(15,15),sharex=True)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in tss.split(data_split):\n",
    "    train = data_split.iloc[train_index]\n",
    "    test = data_split.iloc[test_index]\n",
    "    train.plot(ax=axs[fold], label=f'Train split {fold}', title='Bike Count')\n",
    "    test.plot(ax=axs[fold], label=f'Test split {fold}')\n",
    "    fold += 1\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "plt.legend(['Train split', 'Test split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = ExpandingSlidingWindowSplitter(n_splits=10, fh=7, gap = 0)\n",
    "\n",
    "fig, axs = plt.subplots(10,1, figsize=(15,15),sharex=True)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in tss.split(data_split):\n",
    "    train = data_split.iloc[train_index]\n",
    "    test = data_split.iloc[test_index]\n",
    "    train.plot(ax=axs[fold], label=f'Train split {fold}', title='Bike Count')\n",
    "    test.plot(ax=axs[fold], label=f'Test split {fold}')\n",
    "    fold += 1\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "plt.legend(['Train split', 'Test split'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
